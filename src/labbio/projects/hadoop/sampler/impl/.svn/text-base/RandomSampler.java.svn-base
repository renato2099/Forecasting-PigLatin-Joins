package labbio.projects.hadoop.sampler.impl;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Random;

import labbio.projects.hadoop.executor.LocalInputSamplerExec;
import labbio.projects.hadoop.file.stats.FileStats;
import labbio.projects.hadoop.sampler.Sampler;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.mapreduce.InputFormat;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.TaskAttemptID;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.util.ReflectionUtils;
import org.apache.hadoop.util.StringUtils;
import org.apache.hadoop.util.Tool;

/**
 * Sample from random points in the input.
 * General-purpose sampler. Takes numSamples / maxSplitsSampled inputs from
 * each split.
 */
public class RandomSampler extends Configured implements Tool, Sampler {
	
    
    private double freq;
    private final int maxSplitsSampled;
    
    /**
     * number of samples that will be taken, expressed as a percentage of the whole file.
     */
    private double percentSampled;
    private long numSamples;
    
    /**
     * Path of the file that will be sampled. 
     */
    public Path filePath = null;
	
	/**
	 * Configuration parameters. 
	 * Configuration and Job object.
	 */
    private Configuration conf;
    private Job job;
  
    /**
	 * Create a new RandomSampler sampling all splits.
	 * This will read every split at the client, which is very expensive.
	 * @param pSampleFilePath	file path to sample
	 * @param freq Probability with which a key will be chosen.
	 * @param numSamples Total number of samples to obtain from all selected
	 *                   splits.
	 */
	public RandomSampler(Path pSampleFilePath, double freq, double pPercenSampled) {
		this(pSampleFilePath, freq, pPercenSampled, Integer.MAX_VALUE);
	}

	/**
	 * Create a new RandomSampler.
	 * @param pSampleFilePath	file path to sample
	 * @param freq Probability with which a key will be chosen.
	 * @param numSamples Total number of samples to obtain from all selected
	 *                   splits.
	 * @param maxSplitsSampled The maximum number of splits to examine.
	 */
	public RandomSampler(Path pSampleFilePath, double freq, double pPercenSampled, int maxSplitsSampled) {
		this.filePath = pSampleFilePath;
		this.freq = freq;
		this.percentSampled = pPercenSampled;
		this.maxSplitsSampled = maxSplitsSampled;
	}
  	/**
	 * @param inf Object containing the information about the input.
	 * @param job Object containing the information about the job.
	 * @return the number of rows to be sampled based on the percentage set.
	 * @throws IOException
	 * @throws InterruptedException
	 */
	public void setNumSamples() throws IOException, InterruptedException{
		FileStats fileStatR = new FileStats(this.getFilePath(),new TextInputFormat(),this.job);
		long fileApproxCard = (long) fileStatR.getApproxCardinality();
		//// With this condition we make sure we never ask for more samples than actual rows
		this.numSamples = (long) Math.min(Math.ceil(fileApproxCard * percentSampled),fileApproxCard - fileApproxCard*MAX_SAMPLING_CONST);
	}
  /**
   * Randomize the split order, then take the specified number of keys from
   * each split sampled, where each key is selected with the specified
   * probability and possibly replaced by a subsequently selected key when
   * the quota of keys from that split is satisfied.
   */
  @Override
  @SuppressWarnings("unchecked") // ArrayList::toArray doesn't preserve type
  public Object[] getSample() {
	  
	  // Setting the number of samples to be taken.
	  try {
		  
		setNumSamples();
		TextInputFormat inf = new TextInputFormat();
		List splits = inf.getSplits(job);
		
		// TODO: if the percentage to be sampled is way too big, the arrayList will not work.
		ArrayList samples = new ArrayList();
		FileStats fileStatsR = new FileStats(filePath,inf,job);

		int splitsToSample = Math.min(maxSplitsSampled, splits.size());

		Random r = new Random();
		long seed = r.nextLong();
		r.setSeed(seed);
		//Logger.debug("seed: " + seed);
		
		// shuffle splits
		for (int i = 0; i < splits.size(); ++i) {
			InputSplit tmp = (InputSplit) splits.get(i);
			int j = r.nextInt(splits.size());
			splits.set(i, splits.get(j));
			splits.set(j, tmp);
		}
		
		// our target rate is in terms of the maximum number of sample splits,
		// but we accept the possibility of sampling additional splits to hit
		// the target sample keyset
		for (int i = 0; i < splitsToSample || (i < splits.size() && samples.size() < numSamples); ++i) {
    	
			TaskAttemptContext samplingContext = new TaskAttemptContext(job.getConfiguration(), new TaskAttemptID());
			RecordReader reader = inf.createRecordReader(((InputSplit)splits.get(i)), samplingContext);
			reader.initialize(((InputSplit)splits.get(i)), samplingContext);
      
			while (reader.nextKeyValue()) {
				if (r.nextDouble() <= freq) {
					if (samples.size() < numSamples) {
						String valueSampled = getValueFromLine(reader.getCurrentValue().toString());
						samples.add(valueSampled);
					} 
					else {
			            // When exceeding the maximum number of samples, replace a
			            // random element with this one, then adjust the frequency
			            // to reflect the possibility of existing elements being
			            // pushed out
						int ind = r.nextInt((int)numSamples);
						if (ind != numSamples) {
							String valueSampled = getValueFromLine(reader.getCurrentValue().toString());
							samples.add(ind, valueSampled);
						}
						freq *= (numSamples - 1) / (double) numSamples;
					}
				}//END IF
			}//END WHILE
			
			reader.close();
		}
    
		return (Object[])samples.toArray();
    
	  } catch (IOException e) {
		  // TODO Auto-generated catch block
		  e.printStackTrace();
	  } catch (InterruptedException e) {
		  // TODO Auto-generated catch block
		  e.printStackTrace();
	  }
	  
	  return null;
  }

  	@Override
	public int run(String[] arg0) throws Exception {
		try{
			// configure the job
			this.conf = getConf();
			this.job = new Job(conf, "Random Sampler");
			configureJob(conf,arg0[0]);

			FileInputFormat.addInputPath(job, this.getFilePath());
			
			this.job.setInputFormatClass(TextInputFormat.class);
			this.job.setMapOutputKeyClass(TextInputFormat.class);
			this.job.setJarByClass(ClusterSampler.class);
			
			// TODO MapReduce job for sampling should execute here
			
			return 0;
		}
		catch (Exception e){
			e.printStackTrace();
			return 1;
		}
	}

	@Override
	public String getValueFromLine(String line) {
		int colSampled = getColumnSampled();
		char colSeparator = getColumnSeparator();
		String[] parts = StringUtils.split(line.trim(), StringUtils.ESCAPE_CHAR,colSeparator);
		String value = "";
		
		if (colSampled <= parts.length)
			value = parts[colSampled-1];
		
		return value;
	}
	
	@Override
	public void configureJob(Configuration pConf, String pExecMode){
		if ( pExecMode.equals("true") ) {
			pConf.set("fs.default.name", "file:///");
			pConf.set("mapred.job.tracker", "local");
		}
		else{
			pConf.set("fs.default.name", FS_DEFAULT_NAME);
			pConf.set("mapred.job.tracker", DEFAULT_JOB_TRACKER);
		}
	}
	@Override
	public Job getJobObj(){
		return this.job;
	}
	
	/**
	 * Method to get the index of the column sampled.
	 */
	public int getColumnSampled(){
		return DEFAULT_COLUMN_SAMPLED;
	}
	
	/**
	 * Method to get the column separator.
	 */
	public char getColumnSeparator(){
		return DEFAULT_COLUMN_SEPARATOR;
	}
	
	/**
	 * @param fILEPATH the fILEPATH to set
	 */
	protected void setFilePath(Path fILEPATH) {
		filePath = fILEPATH;
	}
	
	/**
	 * @return the fILEPATH
	 */
	protected Path getFilePath() {
		return filePath;
	}
}